# 6장 키-값 저장소 설계

- 키-값 저장소(key-value store) 키-값 데이터베이스 불리는 비 관계형(non-relational) 데이터베이스



키-값 쌍에서는 키는 유일해야 하며 해당 키에 매달린 값은 키를 통해서만 접근. 키는 일반 텍스트일 수도 있고 해시 값일 수도 있다. 성능상의 이유로 키는 짫을수록 좋다.



### 문제 이해 및 설계 범위 확정

완벽한 설계는 없다. 읽기, 쓰기 그리고 메모리 사용량 사이에 어떤 균형을 찾고, 데이터 일관성과 가용성 사이에서 타협적 결정을 내린 설계를 만들었다면 쓸만한 답안일 것이다.

- 키-값 쌍의 크기는 10KB 이하
- 큰 데이터를 저장할 수 있어야 한다.
- 높은 가용성을 제공. 따라서 시스템은 설사 장애가 있더라도 빨리 응답
- 높은 규모 확장성 제공. 따라서 트래픽 양에 따라 자동적으로 서버 증설/삭제 이루어진다.
- 데이터 일관성 수준은 조정 가능
- 응답 지연시간(latency)이 짧아야 한다.



### 단일 서버 키-값 저장소

한 대 서버만 사용하는 키-값 저장소를 설계하는 것은 쉽다. 직관적인 방법은 키-값 쌍 전부를 메모리에 해시 테이블로 저장

= 이 접근법은 빠른 속도를 보장하긴 하지만 하지만 모든 데이터를 메모리 안에 두는 것이 불가능할 수도 있다는 약점을 갖는다.

개선책으로는 다음과 같다.

- 데이터 압축(compression)
- 자주 쓰이는 데이터만 메모리에 두고 나머지는 디스크에 저장

많은 데이터를 저장하려면 분산 키-값 저장소(distrubuted key-value store)를 만들 필요가 있다.



### 분산 키-값 저장소

분산 키-값 저장소는 **분산 해시 테이블**. 키-값 쌍을 여러 서버에 분산시키는 탓. 분산 시스템을 설계할 때는 

**CAP(Consistency, Availability, Partition Tolerance theorem) 을 이해**

일관성(Consistency), 가용성(Availability), 파티션 감내(Partition Tolerance theorem) 세 가지 요구사항을 동시에 만족하는 분산 시스템을 설계하는 것은 불가능하다.

- **데이터 일관성**: 분산 시스템에 접속하는 모든 클라이언트는 어떤 노드에 접속했느냐에 관계없이 언제나 같은 데이터를 보게 되어야 한다.
- **가용성**: 분산 시스템에 접속하는 클라이언트는 일부 노드에 장애가 발생하더라도 항상 응답을 받을 수 있어야 한다.
- **파티션 감내**: 파티션은 두 노드 사이에 통신 장애가 발생하였음을 의미한다. 파티션 감내는 네트워크에 파티션이 생기더라도 시스템은 계속 동작하여야 한다는 것을 뜻한다.

<img src="https://raw.githubusercontent.com/LenKIM/images/master/2024-03-07/image-20240307204024608.png" alt="image-20240307204024608" style="zoom:50%;" />



- CP 시스템: 일관성과 파티션 감내를 지원하는 키-값 저장소. 가용성을 희생한다.
- AP 시스템: 가용성과 파티션 감내를 지원하는 키-값 저장소. 데이터 일관성을 희생한다.
- CA 시스템: 일관성과 가용성을 지원하는 키-값 저장소. 파티션 감내는 지원하지 않는다. 그러나 통산 네트워크 장애는 피할 수 없는 일로 여겨지므로, 분산 시스템은 반드시 파티션 문제를 감내할 수 있도록 설계되어야 한다.



#### 이상적 상태

이상적 환경이라면 네트워크가 파티션되는 상황은 절대로 일어나지 않을 것이다. n1에 기록된 데이터는 자동적으로 n2과 n3에 복제. **데이터 일관성과 가용성 만족**

<img src="https://raw.githubusercontent.com/LenKIM/images/master/2024-03-07/image-20240307204407484.png" alt="image-20240307204407484" style="zoom:50%;" />

#### 실세계의 분산 시스템

분산 시스템은 파티션 문제를 피할 수 없다. 그리고 파티션 문제가 발생하면 우리는 일관성과 가용성 사이에서 하나를 선택해야 한다.



<img src="https://raw.githubusercontent.com/LenKIM/images/master/2024-03-07/image-20240307204608180.png" alt="image-20240307204608180" style="zoom:50%;" />

n1 및 n2와 통신할 수 없는 상황을 보여주고 있다. 



가용성 대신 일관성을 선택한다면(CP 시스템) 세 서버 사이에 생길 수 있는 데이터 불일치 문제를 피하기 위해 n1과 n2에 대해 쓰기 연산을 중단. 그렇게 하면 가용성이 깨진다.

은행권 시스템은 보통 데이터 일관성을 양보하지 않는다. 



 분산 키-값 저장소를 만들 때는 그 요구사항에 맞도록 CAP 정리를 적용해야 한다.

### 시스템 컴포넌트

키-값 저장소 구현에 사용될 핵심 컴포넌트들 및 기술

- 데이터 파티션
- 데이터 다중화(replication)
- 일관성(consistency)
- 일관성 불일치 해소(inconsistency resolution)
- 장애 처리
- 시스템 아키턱체 다이어그램
- 쓰기 경로(write path)
- 읽기 경로(read path)



**데이터 파티션**

 대규모 애플리케이션의 경우 전체 데이터를 한 대 서버에 욱여넣는 것은 불가능하다. 가장 단순한 해결책은 데이터를 작은 파티션들로 분할한 다음 여러 대서버에 저장하는 것.

데이터를 파티션 단위로 나눌 때는 다음 두 가지 문제를 중요하게 따져봐야 한다.

- 데이터를 여러 서버에 고르게 분산할 수 있는가
- 노드가 추가되거나 삭제될 때 데이터의 이동을 최소화할 수 있는가

안정 해시를 사용하여 데이터를 파티션하면 좋은 점은 다음과 같다.

- 규모 확장 자동화(automatic scaling): 시스템 부하에 따라 서버가 자동으로 추가되거나 삭제되도록 만들 수 있다.
- 다양성(heterogeneity): 각 서버의 용량에 맞게 가상 노드의 수를 조정. 



**데이터 일관성**

여러 노드에 다중화된 데이터는 적절히 동기화가 되어야 한다.



**일관성 모델**

키-값 저장소를 설계할 때 고려해야 할 또 하나의 중요한 요소.

일관성 모델은 데이터 일관성의 수준을 결정.

- 강한 일관성(strong consistency) - 모든 읽기 연산은 가장 최근에 갱신된 결과를 반환. 클라이언트는 과거 데이터를 보지 못한다.
- 약한 일관성(weak consistency) - 읽기 연산은 가장 최근에 갱신된 결과를 반환하지 못할 수 있다
- 결과적 일관성(eventual consistency) -  약간 일관성의 한 형태로, 갱신 결과가 결국에는 모든 사본에 반영(동기화)되는 모델



강한 일관성을 달성하는 일반적인 방법은, 모든 사본에 현재 쓰기 연산의 결과가 반영될 때까지 해당 데이터에 읽기/쓰기를 금지하는 것. 그러나 고가용성 시스템에는 적합하지 않다.

결과적 일관성 모델을 따를 경우 쓰기 연산이 병렬적으로 발생하면 시스템에 저장된 값의 일관성이 깨어질 수 있는데, 이 문제는 클라이언트가 해결해야 한다. 



#### 비 일관성 해소 기법: 데이터 버저닝

데이터를 다중화하면 가용성은 높아지지만 사본 간 일관성이 깨질 가능성이 높다. 버저닝(versioning)과 벡터 시계(vertor clock)는 그 문제를 해소하기 위해 등장

**버저닝**은 데이터를 변경할 때마다 해당 데이터의 새로운 버전을 만드는 것을 의미. 따라서 각 버전의 데이터는 변경 불가능(immutable)하다.

**벡터 시계(vector clock)**는 [서버, 버전]의 순서쌍을 데이터에 매단 것. 어떤 버전이 선행 버전인지, 후행 버전인지, 아니면 다른 버전과 충돌이 있는지 판별하는데 쓰인다.

구체적 사례는 다음과 같다.

<img src="https://raw.githubusercontent.com/LenKIM/images/master/2024-03-07/image-20240307221751986.png" alt="image-20240307221751986" style="zoom:50%;" />

1. 클라이언트가 데이터 D1을 시스템에 기록. 이 쓰기 연산을 처리한 서버는 Sx. 따라서 벡터 시계는  D1[(Sx, 1)]
2. 다른 클라이언트가 데이터 D1을 읽고 D2로 업데이트한 다음 기록. D2는 D1에 대한 변경이므로 D1을 덮어쓴다. 이때 쓰기 연산은 같은 서버 Sx가 처리한다고 가정. 백터 시계는 D2([Sx, 2])로 바뀔 것
3. 다른 클라이언트가 D2를 읽어 D3로 갱신한 다음 기록. 이 쓰기 연산은 Sy가 처리한다고 가정. 백터 시계 상태는 D3([Sx,2], [Sy, 1])
4. 또 다른 클라이언트가 D2를 읽고 D4로 갱싱한 다음 기록. 이때 쓰기 연산은 서버 Sz가 처리한다고 가정. 백터 시계는 D4([Sx, 2], [Sz, 1])일 것
5. 어떤 클라이언트가 D3과 D4를 읽으면 데이터 간 충돌이 있다는 것을 알게 된다. D2를 Sy, Sz가 각기 다른 값으로 바꾸었기 때문. 이 충돌은 클라이언트가 해소한 후에 서버에 기록. 이 쓰기 연산을 처리한 서버는 Sx. 백터 시계는 D5([Sx, 3], [Sy, 1], [Sz, 1]) 

충돌이 일어났다는 것을 어떻게 감지?

**백터 시계를 사용하면 어떤 버전 X가 버전 Y의 이전 버전인지 쉽게 판단할 수 있다. 버전 Y에 포함된 모든 구성요소의 앖이 X에 포함된 모든 구성요소 값보다 같거나 큰지만 보면 된다.**

 백터 시계를 사용해 충돌을 감지하고 해소하는 방법에는 두 가지 분명한 단점. 첫 번째는 충돌 감지 및 해소 로직이 클라이언트에 들어가야 하므로, 클라이언트 구현이 복잡해진다는 것.

 두번째는 [서버: 버전]의 순서쌍 개수가 굉장히 빨리 늘어난다는 점. 이 문제를 해결하려면 그 길이에 어떤 임계치(threshold) 설정. 임계치 이상으로 길이가 길어지면 오래된 순서쌍을 벡터 시계에서 제거하도록 해야 한다. 이렇게 하면 버전 간 선후 관계가 정확하게 결정될 수 없기 때문에 충돌 해소 과정의 효율성이 낮아지게 된다.



**장애 처리**

장애를 어떻게 처리할 것이냐 하는 것은 장애 감지 기법부터 보고 장애 해소 전략을 보자.



**장애 감지**

 모든 노드 사이에 멀티캐스팅(multicating)채널을 구축하는 것이 서버 장애를 감지하는 손쉬운 방법. 하지만 이 방법은 서버가 많을 때는 분명 비효율적.

예를 들어 설명하는 것이 가십 프로토콜(gossip protocal) 같은 분산형 장애 감지(decentralized failure detection)솔루션? 왜??

- 각 노드는 멤버십 목록(membership list)를 유지. 멤버십 목록은 각 멤버 ID와 그 박동 카운터(heartbeat counter)쌍의 목록
- 각 노드는 주기적으로 자신의 박동 카운터 증가
- 각 노드는 무작위로 선정된 노드들에게 주기적으로 자기 박동 카운터 목록을 보낸다.
- 박동 카운터 목록을 받은 노드는 멤버십 목록을 최신 값으로 갱신
- 어떤 멤버의 박동 카운터 값이 지정된 시간 동안 갱신되지 않으면 해당 멤버는 장애상태로 간주.

**일시적 장애 처리**

 가십 프로토콜로 장애를 감지한 시스템은 가용성을 보장하기 위해 필요한 조치를 취한다. 

- 엄격한 정족수(strict quorum) 접근법을 쓴다면 읽기와 쓰기 연산 금지
- 느슨한 정족수(sloppy quorum) 가용성을 높인다. 무슨말이지? 

 네트워크나 서버 문제로 장애 상태인 서버로 가는 요청은 다른 서버가 잠시 맡아 처리한다. 그동안 발생한 변경사항은 해당 서버가 복구되었을 때 일괄 반영하여 데이터 일관성을 보존한다. 이를 위해 임시로 쓰기 연산을 처리한 서버에는 그에 관한 단서(hint)를 남겨둔다. 따라서 이런 장애 처리 방안을 단서 후 임시 위탁(hinted handoff) 기법이라 한다.



(중간 스킵)

**시스템 아키텍쳐 다이어그램**

- 클라이언트는 키-값 저장소가 제공하는 두 가지 단순한 API, 즉 get(key)및 put(key, value)와 통신
- 중재자(coordinator)는 클라이언트에게 키-값 저장소에 대한 프록시(proxy)역할을 하는 노드
- 노드는 안정 해시(consistent hash)의 해시 링(hash ring)위에 분포

<img src="https://raw.githubusercontent.com/LenKIM/images/master/2024-03-09/image-20240309080351893.png" alt="image-20240309080351893" style="zoom:50%;" />

- 노드는 자동으로 추가 또는 삭제할 수 있도록, 시스템은 완전히 분산된다(decentralized).
- 데이터는 여러 노드에 다중화된다.
- 모든 노드가 같은 책임을 지므로, SPOF(Single Point of Failure)는 존재하지 않는다.

완전히 분산된 설계를 채택.

<img src="https://raw.githubusercontent.com/LenKIM/images/master/2024-03-23/image-20240323210234711.png" alt="image-20240323210234711" style="zoom:50%;" />

#### 쓰기 경로

1. 쓰기 요청이 커밋 로그 파일에 기록된다
2. 데이터가 메모리 캐시에 기록된다
3. 메모리 캐시가 가득차거나 임계치에 도달하면 데이터는 디스크에 있는 SSTable에 기록된다(Sorted-String Table)



#### 읽기 경로

1. 데이터가 메모리 있는지 검사한다. 없으면 2로 이동
2. 데이터가 메모리에 없으므로 Bloom filter를 검사한다
3. Bloom filter를 통해 어떤 SSTable에 키가 보관되어 있는 지 알아낸다
   - https://ko.wikipedia.org/wiki/%EB%B8%94%EB%A3%B8_%ED%95%84%ED%84%B0
4. SSTable에서 데이터를 가져온다
5. 해당 데이터를 클라이언트에게 반환한다



<img src="https://raw.githubusercontent.com/LenKIM/images/master/2024-03-23/image-20240323203254450.png" alt="image-20240323203254450" style="zoom:33%;" />

## 무엇을 알아야 할까?

- Key-Value Store 는 언제 사용할까?

```
- 키-값 쌍의 크기는 10KB 이하
- 큰 데이터를 저장할 수 있어야 한다.
- 높은 가용성을 제공. 따라서 시스템은 설사 장애가 있더라도 빨리 응답
- 높은 규모 확장성 제공. 따라서 트래픽 양에 따라 자동적으로 서버 증설/삭제 이루어진다.
- 데이터 일관성 수준은 조정 가능
- 응답 지연시간(latency)이 짧아야 한다.
```

- 위 요구사항에 부합하는 Key-Value Store 는 만들기 쉬울까?
  - 쉽지 않다. 그렇기 때문에 공부하는 것.
- **가십 프로토콜**에 대한 추가 내용
  - Kafka 에는 zookeeper
    - https://zookeeper.apache.org/doc/r3.9.1/zookeeperProgrammers.html#ch_zkDataModel
    - https://rok93.tistory.com/entry/%EC%A3%BC%ED%82%A4%ED%8D%BCZookeeper%EB%9E%80
  - ElasticSearch 는 Full mash Network
    - https://www.elastic.co/kr/blog/found-elasticsearch-networking 
  - 누군가 구현한 코드?
    - https://github.com/ympons/gossip-protocol-java
    - https://github.com/LimHanGyeol/distributed-key-value-store
- 최종적 일관성에 대한 이해





| 기술                                                         | 무슨 목표/문제를 위해? |
| ------------------------------------------------------------ | ---------------------- |
| 안정 해시를 사용해 서버들의 부하 분산                        |                        |
| 데이터를 여러 데이터센터에 다중화                            |                        |
| 버저닝 및 벡터 시계를 사용한 충돌 해소                       |                        |
| 안정해시                                                     |                        |
| 정속수 합의(quorum consensus)                                |                        |
| 느슨한 정족수 프로토콜(sloppy quorum)과 단서 후 임시 위탁(hinted handoff) |                        |
| 머클 트리(Merkel tree)                                       |                        |



<img src="https://raw.githubusercontent.com/LenKIM/images/master/2024-03-23/image-20240323211359987.png" alt="image-20240323211359987" style="zoom:50%;" />
