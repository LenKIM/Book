## 11장 내용
- 11.1 성능에 대해
- 11.2 암달의 법칙
- 11.3 스레드와 비용
- 11.4 락 경쟁 줄이기
- 11.5 예제: Map 객체의 성능 분석
- 11.6 컨텍스트 스위치 부하 줄이기

## 11.1 성능에 대해
- 스레드를 사용하는 가장 큰 목적은 바로 성능을 높이고자 하는 것이다.
- 스레드를 사용하면 시스템의 자원을 훨씬 효율적으로 활용 할 수 있고, 어플리케이션으로 하여금 시스템이 갖고 있는 능력을 최대한 사용하게 할 수 있다.
- 그와 동시에 기존 작업이 실행되고 있는 동안에라도 새로 등록된 작업을 즉시 실행할 수 있는 준비를 갖추고 있기 때문에 애플리케이션의 응답 속도를 향상 시킬 수 있다.
- 11장에서는 병렬프로그래밍의 성능을 분석하고, 모니터링하고, 그결과로 성능을 향상 시킬 수 있는 방법에 대해 알아본다.
- 프로그램이 정상적으로 동작하도록 만들어 놓고 난 다음 프로그램이 빠르게 동작하도록 만드는 편이 낫다고 본다.
- 예상했던 성능 기준이 있엇다면, 그기준에 미치지 못할 경우에만 성능 문제를 살펴보는것도 충분하다.
- 병렬 어플리케이션을 성계하는 동안에는 성능을 최대한으로 끌어 올리는 일이 큰 부분을 차지하지 않을때가 많다.

- 성능을 높인다는것은 더 적은 자원을 사용하면서 더많은 일을 하도록 한다는 말
- 자원이라는 단어에는 여러 가지 뜻이 있을 수 있는데, 처리해야 할 작업이 있을대 CPU, 메모리, 네트웍속도, 디스크속도, 데이터베이스 처리속도, 디스크 용량등의 자원 가운데 어느것이 될지 모르지만 항상 모자라는 부분이 발생할것이다.
- 어떤 작업을 실행 할 때 충분하지 못한 특정 자원 때문에 성능이 떨어지는 현상이 나타난다면, 작업의 성능이 해당 자원에 좌우된다고 한다.
    - CPU에 좌우 될 수 도 있고, 데이터베이스 속도에 좌우될 수 도 있다.
- 여러개의 스레드를 사용하려면 항상 단일 스레드를 사용할때보다 성능상의 비용을 지불해야만 한다
    - 스레드간의 작업 내용을 조율하는데 필요한 오버헤드
    - 컨텍스트 스위칭이 자주 발생한다는점
    - 스레드를 생성하거나 제거하는 일이 빈번하다는점
    - 여러 스레드를 효율적으로 스케쥴링 해야되는것 모두 비용
- 이와 같은 비용을 지불한다고 해도 스레드를 효율적으로 잘 적용하면 성능이나 응답성이 높아지고 처리 용량도 커지는 여러 장점을 얻을 수 있다.
- 반대로 잘못 설계된 병렬 어플리케이션은 순차적으로 작업을 처리하는 프로그램보다 오히려 느리게 동작하는 경우도 간혹 생긴다.

더나은 성능을 목표로 해서 프로그램이 병렬로 동작하도록 만들 때는 두가지 부분을 우선적으로 생각해야한다.
- 프로그램이 확보 할 수 있는 모든 자원을 최대한 활용해야하고, 남는 자원이 생길때마다 그 자원 역시 대한 활용할 수 있도록 해야한다
- 프로그램의 성능을 모니터링하는 관점에서 얘기해보자면, 앞에서 언급한 부분은 CPU가 최대한 바쁘게 동작해야 한다는것과 동일하게 생각 할 수 있다.
- 프로그램에서최 스레드를 활용하면 작업을 자게 나눠 시스템에 꽂힌 CPU 가 충분히 열심히 동작해야 할 만큼의 작업을 싱행시켜 노는 CPU가 없을 만큼 작업 실행 성능을 높일 수 있다.

### 성능 대 확장성
성능 대 확작성
- 애플리케이션의 성능은 여러가지 측면에서 자료를 수집해 측정할 수 있는데, 이를테면 서비스 시간, 대기시간, 처리량, 효율성, 확장성, 용량 등의 수치를 뽑아 낼 수 있다.
    - 특정 작업을 처리하는 속도가 얼마나 빠르냐
    - 동일한 자원을 갖고 있을 때 얼마나 많은 양의 일을 할 수 있는지
- **확장성은 CPU, 메모리, 디스크 , I/O 처리 장치등의 추가적인 장비를 사용해 처리량이나 용량을 얼마나 쉽게 키울 수 있는지를 말한다.**
- 확장성을 목표로 튜닝을 한다면 처리해야 할 작업을 병렬화해 시스템의 가용 자원을 더 많이 끌어다 사용하면서 더 많은 일을 처리 할 수 있도록 하는 방법을 많이 사용하게 된다.
- 이처럼 성능이라는 단어에 포함된 얼마나 빠르게와 얼마나 많이라는 두가지의 의미는 완전히 다른뜻을 가지며, 어떤경우에는 화합할수 없는 상황도 발생
- 단일 스레드 어플리케이션에서 사용하던 성능방안은 대부분 확장성 측면에서 효과적이지 않다.
- 3티어모델(프레제이션티어,비지니스로직티어,스토리지 티어)을 보면 시스템 확장성을 높이도록 변경할때 성능의 측면에서 얼마나 많은 손해를 보년경우가 많은지 쉽게 알수 있다.
- 단일 어플리케이션이 성능이 훨씬 나을 가능성이 많음
    - 네트워크 지연현상 X
    - 연산작업을 서로 다른 추상적인 계층을 통과시키가며 처리하는데 드는 부하가 적기때문에
- 단일구조 어플리케이션 처리할수 있는 최대부하를 넘어서는 작업량을 감당해야하는 순간이오면 문제 심각
    - 처리용량을 단시간에 급격하게 증가시키는 일 어려움
    - 시스템 자원을 계속투입, 서비스 시간이 훨씬 길어지거나 단위 작업당 필요한 하드웨어 자원의 양을 크게 늘어나는 일 감수
- 서버 어플리케이션을 만들때는 성능의 여러가지 측면까운데 얼마나 빠르게라는 측면보다 얼마나 많이 라는 측명 즉 확장성과 처리량과 용량이라는 세개의 측면을 훨씬 중요하게 생각하는 경우 많다
    - 사용자와의 직접적인 사용하 작용이 일어나는 어플리케이션이라면 대기시간이라는 값이 중요
    - 대기시간을 줄여야 사용자가 진행하는 상태를 보며 실제로 무슨일이 일어나는지 기다리는 시간이 줄어듬
- 11장에서는 단일스레드 상황에서 성능보다는 확장성을 중점적으로 다룬다

### 성능 트레이드 오프 측정
- 공학적인 모든 선택의 순간에는 항상 트레이드 오프가 존재
- ex) 다리 건설 튼튼하게 지으면 안정성 높아지지만 돈많이듬
- 트레이드 오프에서 어떤부분을 선택해야 할지 결정하는데 필요한 정보가 그다지 충분하지 않은 경우가 많음
- ex) 퀵소트 알고리즘 대량의 자료를 정렬할때는 효율 자료의 양의 많지않을때는 버블소트 알고리즘이 더 효율적임
    - 요구사항에 따라 효율적인게 달라짐
- 요구사항을 충분히 받지 못해 정확하게 구현하지 못한 상태에서 효과가 없다고 판단하는 경우가 많ㅇ끼때문에 최적화 기법을 잘 적용하지 못하는 원인이됨
- **최적화 기법을 너무 이른 시점에 적용하지 말아야한다. 일단 제대로 동작하게 만들고 난 다음에 빠르게 동작하도록 최적화 해야하며, 예상한것보다 심각하게 성능이 떨어지는 경우에만 최적화 기법을 적용하는것으로 충분하다.**
- 공학적인 결정을 내려야하는 시점에는 어떤 효과를 얻고자 할때 다른 비용을 지출해야만 할수 있고, 또 어떤 경우에는 안정성을 확보하기 위해 비용을 지불해야 될수도 있다
- 성능을 높이기 위한 대부분의 결정사항에는 다양한 변수가 관여하곤 하고 처한 상황에 따라 결정사항이 크게 달라진다.

다음과 같은 질문에 답할 필요가 있음
- 빠르다 단어가 무엇을 의미하는가?
- 어떤 조건을 갖춰야 이방법이 실제로 빠르게 동작할것인가? 부하가 적을때? 아니면 부하가 걸릴때? 데이터가 많을때? 아니면 적을때? 이런질문에 대한 대답에 명확한 수치를 보여줄수 있는가?
- 위의 조건에 해당하는 경우가 얼마나 많이 발생하는가? 이런질문에 대한 대답에 명확한 수치를 보여줄수 있는가?
- 조건이 달라지는 다른 상황에서도 같은 코드를 사용할수 있는가?
- 이방법으로 성능을 개선하고자 할때 숨겨진 비용, 즉 개발 비용이나 유지보수 비용이 증가하는 부분이 어느정도인지? 과연 그런 비용을 감수하면서 까지 성능 개선작업을 해야하는가?

- 병렬프로그래밍에서 발생하는 오류의 가장큰원인이 바로 성능을 높이려는 여러가지 기법에서 비롯된다고 봐도 무리가아니다.
- 버그의 원인이 될 가능성이 조금이라도 있는 위험도 높은 코드는 매우 주의깊게 살펴봐야한다.
- 성능을 높이기위해 안전성을 떨어뜨리는것은 최악의 상황이며, 결국 안전성과 성능 둘다 놓치는 결과를 얻는다.
- 따라서 성능을 튜닝하는 모든 과정에 항상 성능 목표에 대한 명확한 요구사항이 있어야한다
- 어느 부분 튜닝, 어느 시점에서 튜닝 그만둬야하는지 판단 할 수 있다.
- 실제와 같은 사용자 부하의 특성을 동일하게 나타낼 수 있는 성능 측정 도구가 있어야한다.
- 그리고 성능 튜닝 후 원하는 목표치를 달성했는지 다시한번 측정값을 뽑아야 한다.
- **추측하지 말고 실제로 측정해보라**

- 시장에 나온 성능 측정용 제품을 보면 소프트웨어 성능을 세밀하게 측정해주고 성능을 떨어뜨리는 병목이 어디에 있는지 눈으로 직접 볼수 있게 해준다.
- 예를들어 perfbar

## 11.2 암달의 법칙
- 일부 작업은 자원을 많이 투입하면 더 빨리 처리 할 수 있다.
    - ex) 곡식을 추수 할땐
- 대부분 병렬 프로그램에는 병렬화 할 수 있는 작업과 순차적으로 처리해야 하는 작업이 뒤섞인 단위 작업의 덩어리를 갖고 있다. 암달의 법칙을 사용하면 병렬의 작업과 순차적의 비율에 따라 하드웨어 자원을 추가로 투입했을때 이론적으로 속도가 얼마나 빨라질지에 대한 예측값을 얻을 수 잇다.
- 암달의 법칙에 따르면 순차적으로 실행돼야 하는 작업의 비율을 F 라고 하고 하드웨어에 꽂혀있는 프로세서의 개수를 N이라고 할때 수식에 해당하는 정도까지 속도를 증가 시킬수 있다.
- N이 무한대까지 증가할수록 속도 증가량은 최고 1/F까지 증가한다.
- 1/F 라는 속도증가량은 순차적으로 실행돼야 하는 부분이 전체 작업 50%를 차지한다고 할때 프로세서를 아무리 많이 꽂는다해도 겨우 두배 빨라진다는 결과
- 그리고 순차적으로 실행해야 하는 부분이 전체의 10% 해당한다면 최고 10배의 속도를 증가 시킬 수 있다고 예측할 수 있다.
- 암달의 법칙을 활용하면 자업을 순차적으로 처리하는 부분이 많아 질때 느려지는 정도가 얼마만큼인지 수치화 할 수 있다.
- ex) 하드웨어 CPU 가 10개 꽂혀있을때 10% 순차작업을 갖고 있는 프로그램은 최고 5.3배 만큼 속도를 증가
    - cpu 활용도는 5.3배/10 = 0.53 즉 53% 시킬수 있다
- ex) 하드웨어 CPU 가 100개 꽂혀있을때 10% 순차작업을 갖고 있는 프로그램은 최고 9.2배 만큼 속도를 증가
    - cpu 활용도는 9.2배/100 = 0.092 즉 9.2% 시킬수 있다
- 속도를 최대 10배 까지 증가시키려면 CPU 활용도가 너무나 비효율적으로 떨어질수 밖에 없음

![KakaoTalk_Photo_2021-06-12-15-46-59](https://user-images.githubusercontent.com/38197944/121767774-76e1b080-cb95-11eb-9060-eb5434f49569.jpeg)
- 암달의 법칙에 따르면 프로세서의 개수가 증가하면 할수록 순차적으로 실행해야하는 부분이 아주 조금이라도 늘어나면 프로세서 개수에 비해 얻을 수 있는 속도 증가량이 크게 떨어진다.
- 멀티 프로세서 시스템에서 어플리케이션을 실행 할 때 속도가 얼마만큼 빨리질것인지 예측해보려면 내부에서 순차적으로 처리해야 하는 작업이 얼마나 되는지 먼저 확인해야한다.

~~~java
public class WorkerThread extends Thread {
    private final BlockingQueue<Runnable> queue;

    public WorkerThread(BlockingQueue<Runnable> queue) {
        this.queue = queue;
    }

    public void run() {
        while (true) {
            try {
                Runnable task = queue.take();
                task.run();

            } catch (InterruptedException e) {
                break; /*스레드를 종료시킨다.*/
            }
        }
    }
}
~~~
- 작업 큐에 대한 순차적인 접근
- 내부적으로 살펴보면 순차적으로 처리해야만 하는 부분이있다
- 바로 작업큐에서 작업을 하나씩 뽑아내는 부분이다.
- ex) 큐의 상태를 안정적으로 유지하고자 락을 사용했다면, 특정 스레드가 큐에서 작업을 하나 뽑아내는 그시점에 역시 큐에서 작업을 가져가고자 하는 다른 모든 스레드는 큐를 독점적으로 사용할 수 있을때까지 대기해야만 한다. 따라서 작업큐와 관련된 부분에서는 ㅅ프로그램이 순차적으로 처리 될 수 밖에 없다.
- 단일 작업 하나가 실행되는 시간에는 Runnable 실행하는 시간뿐만 아니라 공유돼 있는 작업 큐에서 작업을 뽑아내는 필요한 시간도 포함돼 있다.
- 작업 큐로 LinkBlockingQueue 를 사용하고 있다면 LinkedList 보다 훨씬 확장성이 좋은 알고리즘을 사용하고 있기 때문에 시간이 훨씬 적게든다. 하지만 데이터를 한군데에 공유해두고 사용하는 모든 부분은 항상 순차적으로 처리해야만 한다.
- 자바 Runnable 작업은 항상 실행결과를 로그파일에 적어두거나 특정 데이터 구조에 실행결과를 쌓아두도록 돼있다
    - 이부분 역시 순차적으로 처리해야만 하는 부분
- **모든 병렬 프로그램에는 항상 순차적으로 실행돼야되는 부분이 존재한다. 만약 그런 부분이 없다고 생각한다면, 프로그램 코드를 다시 한번 들여다보라.**

### 예제 프레임웍 내부에 감춰져 있는 순차적 실행 구조

![KakaoTalk_Photo_2021-06-12-15-46-54](https://user-images.githubusercontent.com/38197944/121767776-7812dd80-cb95-11eb-95d7-3c9af065b975.jpeg)
- Queue 가 있을때 여러 스레드가 값을 하나씩 뽑아낸 다음 뭔가 작업을 실행하는 일을 계속 반복하는 간단한 어플리케이션의 실행 속도를 측정 결과를 볼 수 있다.
- 스레드 안전성이 보장된 두가지 Queue 구현 클래스 성능을 한번에 볼 수 있음
- 하나는 synchronizedList 메소드로 동기화한 LinkedList 클래스이고, 다른 하나는 ConcurrentLinkedQueue 클래스 이다.
- 각 실행단위가 동일한 양의 작업을 처리한다고 할때 단순히 적절한 큐 구현 클래스를 사용하는것만으로도 확장성을 크게 높일수 있다는 사실을 알수 잇다.
- ConcurrentLinkedQueue 클래스의 처리량은 계속 증가하다가 프로세서의 개수에 해당하는 수치에 다다르면 더이상 증가하지 않고 유지하는 형태를 보인다.
- 반대로 동기화된 LinkedList 클래스의 성능은 스레드가 3개 정도까지는 증가하다가 그이후에는 동기화 관련 부하가 늘어서 성능이 떨어진다.
- 차이점이 발생하는 원인은 바로 두개의 큐 구현 클래스가 작업을 순차적으로 처리하는 정도의 차이점에 원인이 있음을 알수 있다. 동기화된 LinkedList 클래스는 전체 큐의 상태에 하나의 락으로 동기화 하고 있으며, 따라서 offer 나 remove 메소드를 호출하는 동안 전체 큐가 모두 락에걸린다.
- 반대로 ConcurrentLinkedQueue 클래스는 정교한 큐 알고리즘 즉 개별링크 포인터마다 단일연산으로 업데이트 하는 방법을 사용해 대기상태에 들어가는 경우를 최소화 한다.
- 동기화된 LinkedList 는 추가 작업과 삭제작업이 모두 순차적으로 처리되어야 하지만 ConcurrentLinkedQueue 에서는 개별 포인터에 대한 업데이트 연산만 순차적으로 처리하면 된다.

### 정성적인 암달의 법칙 적용 방법
- 암달의 법칙을 사용하면 프로그램 내부에서 순차적으로 처리돼야만 하는 부분의 비율을 알고 있을때, 하드웨어를 추가함에 따라 얼만큼 처리 속도가 증가할 것인지를 수치화 해서 예측 할 수 있다.
- cpu가 많이 보급되면서 이제는 많은 프로세서를 장착한 시스템을 어렵지 않게 생각하게 되었다 이런 환경에서 장착된 하드웨어에서 확장성이 충분하다고 생각됐던 알고리즘이 훨씬 규모가 큰 시스템을 대상으로 본다면 지금까지 알지 못했던 확장성에서의 병목을 맞닥뜨리게 될 가능성도 높다.
- 어느 시점쯤에서 확장성의 한계가 나타날것인지 예측해볼수있다. 락의 적용 범위를 줄이는 방법 즉 락 분할 방법과 락 스트라이핑에 대해서 알아볼것이다.
- 락의 적용 범위를 줄이는 방법은 암달의 법칙이라는 측면에서 바라보면 락을 두개로 분할하는 정도로 다수의 프로세서를 충분히 활용하기 어렵다는 결론을 얻을수 있다.
- 하지만 락스트라이핑 방법을 사용할때는 프로세서의 수가 늘어남에 따라 분할 개수를 같이 증가시킬수 있기때문에 확장성을 얻을 수 있는 믿을만한 방법이라고 할 수 있다.


## 11.3 스레드와 비용
- 스레드를 사용하는 경우 병렬로 실행함으로써 얻을 수 잇는 이득이 병렬로 실행하느라 드는 비용을 넘어서야 성능을 향상 시킬수 있다.

### 컨텍스트 스위칭
- 하나의 스레드가 실행되다가 다른 스레드가 실행되는 순간 컨텍스트 스위칭이 일어난다.
- 컨텍스트 스위칭이 일어나는 상세한 구조를 보면 먼저 현재 실행중인 스레드의 실행상태를 보관해두고, 다음번에 실행되기로 스케쥴 된 다른 스레드의 실행상태를 다시 읽어들인다.
- 스레드를 스케쥴링을 하려면 운영체제와 JVM 내부의 공용 자료 구조를 다뤄야 한다는 문제가 있다.
- 운영체제와 JVM 역시 스케쥴 프로그램 스레드가 사용하는 것과 같은 CPU를 사용하고 있다.
- 따라서 운영체제나 JVM이 CPU를 많이 사용하면 할수록 실제 프로그램 스레드가 사용할 수 있는 CPU양을 줄어든다.
- 컨덱스트가 변경되면서 다른 스레드를 실행하려면 해당 스레드가 사용하던 데이터가 프로세서의 캐시 메모리에 들어 있지 않을 확률도 높다. 그러면 캐시에서 찾지 못한 내용을 다른 저장소에서 찾아와야 하기 때문에 원래 예정된 것 보다 느리게 실행되는 셈이다.
- 이런 경우에 대비하고자 대부분의 스레드 스케줄러는 실행 대기중인 스레드가 밀려있다고 해도 현재 실행중인 스레드에게 최소한의 실행시간을 보장해주는 정책을 취하고 있다.
- 그러면 컨텍스트 스위칭에 들어가는 시간과 비용을 나누는 효과를 볼 수 있고, 그결과 인터럽트 받지 않고 실행할수 있는 최소한의 시간을 보장받기떄문에 전체적인 성능이 향상되는 효과를 볼 수 있다.
- 대기 상태에 들어가는 연산을 많이 사용하는 프로그램(블로킹 I/O 사용하거나, 락 대기 시간이 길거나, 상태변수 값을 기다리는)은 CPU를 주로 활용하는 프로그램보다 컨텍스트 스위칭 횟수가 훨씬 많아지고 따라서 스케줄링 부하가 늘어나면서 전체적인 처리량이 줄어든다.
    - 넌블로킹 알고리즘을 사용하면 컨텍스트 스위칭에 소모되는 부하를 줄일 수 있다.
- 컨텍스트 스위칭 필요한 부하와 비용을 플랫폼마다 다름 대략 5000~10000 클럭 사이클 또는 마이크로 초 동안의 시간을 소모한다고 알려져 있다.
- 유닉스 시스템의 vmstat 명령이나 윈도우 시스템의 perfmon 유틸리티를 사용하면 컨텍스트 스위칭 일어난 횟수 확인 할 수 있음
- 커널 활용도가 10%가 넘는 높은 값을 갖기오 있다면 스케쥴링 부하가 걸린다는 의미 아마도 어플리케이션 내부에서 I/O나 락관련 동기화 부분때문에 대기 상태에 들어가는 부분이 원인일 가능성이 높다.

### 메모리 동기화
- synchronized 와 volatile 키워드를 사용해 얻을 수 있는 가시성을 통해 메모리 배리어라는 특별한 명령어를 사용 할 수 있다.
- 메모리 배리어는 캐시를 플러시하거나 무효화 하고, 하드웨어와 관련된 쓰기 버퍼를 플러시하고, 실행 파이프 라인을 늦출 수도 있다.
- 메모리 배리어를 사용하면 컴파일러가 제공하는 여러가지 최적화 기법을 제대로 사용할 수 없게 돼 간접적인 성능 문제를 가져 올 수 있다.
    - 메모리 배리어를 사용하면 명령어 재배치를 대부분 할 수 없기 때문이다.
- 동기화가 성능에 미치는 영향을 파악하려면 동기화 작업이 경쟁적인지, 비경쟁적인지를 확인해야한다.
- synchronized 키워드가 동작하는 방법은 비경쟁적인 경우에 최적화 돼있기 때문에 빠른경로의 비경쟁적인 동기화 방법은 대부분의 시스템에서 20~250 클럭 사이클을 사용한다고 알려져있다.
- 물론 클럭 사이클을 전혀 사용하지 않는것은 아니지만, 전반적인 어플리케이션 성능 측면에서 봤을때 영향이 없다고 할 수 있음.

~~~java
synchronized (new Object()){
    //작업 진행
}
~~~
- 아무런 의미가 없는 동기화 구문 이런코드는 금물! JVM 락을 사용하지 않는다.
- 최근 사용하는 JVM 은 대부분 다른 스레드와 경쟁할 가능성이 없다고판단되는 부분에 락이 걸려 있다면 최적화 과정에서 해당락을 사용하지 않도록 방지하는 기능을 제공하기도한다.
- 예를 들어 락을 거는 객체가 특정 스레드 내부에 한정되어있다면 해당 락을 다른 스레드에서 사용하며 경쟁 조건에 들어 갈 수 없기 때문에 JVM 은 해당 락은 무시하고 넘어간다.

~~~java
public String getStoogeNames(){
    List<String> stooges = new Vector<String>();
    stooges.add("Moe");
    stooges.add("Larry");
    stooges.add("Curly");
    return stooges.toString();
}
~~~
- 락 제거 대상
- 정교하게 만들어진 JVM의 경우 유출 분석을 통해 로컬 변수가 외부로 공개된적이 있는지 없는지 다시 말해 해당 변수가 스레드 내부에서만 사용되는지 판단하기도 한다.
- 허술한 JVM 에서 위의 코드를 사용할경우 add, toString을 호출하는 부분에서  4번 총 락을 잡게 된다.
- 정교한 고급컴파일러와 JVM은 stooges가 메소드 외부에 유출된 적이 없다는것을 판단하고 락을 4번이나 잡지 않는다. 빠르게 실행시킨다.
- 유출 분석을 사용하지 않는 경우라면 락확장 즉 연달아 붙어있는 여러개의 synchronized 블록을 하나의 락으로 묶는 방법을 사용하기도 한다.
- 락 확장 기능을 갖고있는 JVM 에서 위의 코드 실행한다면 add 메소드 3번 toString 호출 한번에 묶어 락을 한번만 확보하고 해제한다.
- 락 확장 기능을 사용하는 JVM은 락을 확보하고 해제하는 걸리는 시간과 synchronized 블록 내부의 작업에 걸리는 시간을 살펴보고 확장하는 것이 효율적이라고 판단되는 경우에만 확장하기도 한다.
- 락 확장 방법을 사용하면 동기화 관련 부하를 줄이는데 도움, 최적화 모듈이 좀더 큰 단위의 블록을 대상으로 추가적인 최적화 작업을 진행할 기회가 생기기도한다.
- **경쟁 조건에 들어가지 않는 동기화 블록에 대해서는 그다지 걱정하지 않아도 좋다. 동기화 블록의 기본적인 구조가 상당히 빠르게 동작 할 뿐만 아니라, JVM 수준에서 동기화와 관련한 추가적인 최적화 작업을 진행하기 떄문에 동기화 관련 부하를 줄이거나 아예 없애주기도 한다. 대신 경쟁조건이 발생하는 동기화 블록을 어떻게 최적화 할지 대해서 고민하자.**
- 특정 스레드에서 진행되는 동기화 작업으로 인해 다른 스레드의 성능이 영향을 받을 수 있다.
- 동기화 작업은 공유돼있는 메모리로 통하는 버스에 많은 트래픽을 유발 하기 때문이다. 공유 메모리로 통하는 버스는 제한적인 대역폭을 갖고 있으며 여러개의 프로세서가 공유한다.
- 특정 스레드가 동기화 작업을 진행하느라 공유 메모리로 통하는 버스의 대역폭을 꽉 잡고 있다면, 동기화 작업을 진행해야 할 다른 스레드는 성능이 떨어 질 수 밖에 없다.
-
### 블로킹
- 락을 확보하지 못한 스레드는 항상 대기 상태에 들어가야한다.
- JVM은 스레드를 대기 상태에 둘때 두가지 방법을 사용한다.
- 첫번째는 스핀대기 spin waiting 락을 확보할때까지 계속 재시도하는 방법
- 두번째 방법은 운영체제가 제공하는 기능을 사용해 스레드를 실제 대기 상태로 두는 방법
- 두개 방법 가운데 어느쪽이 효율적이냐는 답은 컨텍스트 스위칭에 필요한 자원의 양과 락을 확보할때까지 걸리는 시간에 크게 좌우된다. 대기 시간을 놓고 보면 대기 시간이 짧은 경우에는 스핀 대기 방법이 효과적이고, 대기 시간이 짧은 경우네는 스핀 대기 방법이 효과적이고, 대기시간이 긴경우에는 운영체제의 기능을 효출하는 편이 효율적이라고 한다.
- 일부 JVM은 이전에 실행된 패턴을 분석한 결과를 놓고 두가지 방법중에 효과적인 방법을 선택해 사용하기도 하지만 대부분의 경우 운영체제의 기능을 호출하는 방법을 사용한다.
- 락을 확보하지 못했거나 I/O 관련 작업을 사용중이라거나 기타 여러가지 조건에 걸려들어 스레드가 대기상태에 들어갈때는 두번의 컨텍스트 스위칭 작업이 일어나며, 이 과정에는 운영체제와 각종캐시등의 모듈이 연결돼 있다.
- 첫번째 컨텍스트 스위칭은 실행하도록 할당된 시간이전에 대기 상태에 들어가느라 발생하는것이고, 두번째는 락이나 기타 필요한 조건이 충족됬을때 다시 실행상태로 돌아오는 컨텍스트 스위칭이다.

## 11.4 락 경쟁 줄이기
- 락 경쟁을 줄이면 줄일수록 확장성과 성능을 함께 높일 수 있다.
- **병렬 어플리케이션에서 확장성에 가장 큰 위협이 되는 조냊는 바로 특정 자원을 독점적으로 사용하도록 제한하는 락이다.**
- 락을 두고 발생하는 경쟁상황에는 크게 두가지 원인
    - 즉 락을 얼마나 빈번하게 확보하려고 하는지
    - 한번 확보하고 나면 해제할때까지 락을 얼마나 오래 사용하는지가 중요한 요인
- 락을 필요로 하는 굉장히 많은 수의 스레드가 경쟁하고 있다면 락을 확보하지 못한 다수의 스레드가 계속 대기 상태에 머물러야 하며, 심각한 경우에는 작업할 내용이 쌓여 있음에도 불구하고 CPU는 실제로 놀고 있을 가능성도 있다

락 경쟁 조건을 줄일 수 있는 몇가지 방법이 있다
- 락을 확보한채로 유지되는 시간을 최대한 줄여라
- 락을 확보하고자 요청하는 횟수를 최대한 줄여라
- 독점적인 락 대신 병렬성을 크게 높여주는 여러가지 조율 방법을 사용하라.

### 락 구역 좁히기
- 락 경쟁이 발생할 가능성을 줄이는 효과적인 방법 가운데 하나는 바로 락을 유지하는 시간을 줄이는 방법이다.
- 락이 꼭 필요하지 않은 코드를 synchronized 블록 밖으로 뽑아내어 락이 영향을 미치는 구역을 좁히면 락을 유지하는 시간을 줄일 수 있다.
- 특히 I/O 작업과 같이 대기시간이 발생할 수 있는 코드는 최대한 synchronized 블록 밖으로 끄집어 내자
~~~java
@ThreadSafe
public class AttributeStore {
    @GuardedBy("this") private final Map<String,String> attributes  = new HashMap<>();
    
    public synchronized boolean userLocationMatches(String name, String regexp){
        String key = "users."+name+".location";
        String location = attributes.get(key);
        if(location == null)
            return false;
        else
            return Pattern.matches(regexp, location);
    }
}
~~~
- 필요 이상으로 락을 잡고 있는 모습
- synchronized 로 막아야 할 부분은 Map.get 메소드를 호출하는 부분뿐이다.

~~~java

@ThreadSafe
public class BetterAtributeStore {
    @GuardedBy("this") private final Map<String,String> attributes  = new HashMap<>();

    public boolean userLocationMatches(String name, String regexp){
        String key = "users."+name+".location";
        String location;
        synchronized (this) {
            location = attributes.get(key);
        }
        if(location == null)
            return false;
        else
            return Pattern.matches(regexp, location);
    }
}
~~~
- 락 점유 시간 단축
- 위에 코드에 비해 락 점유 시간을 엄청나게 줄인 버젼이다.
- 믄지얄을 연결해 키를 생성하는 작업이나 정규표현식 문제열을 매치하는 작업을 공유된 데이터를 사용하지 않으므로 락 블록 내부에서 실행시킬 필요가 없다.
- 이와 같이 공유데이터를 사용하지 않는 부분을 락 블록 밖으로 뽑아내 확보한 상태로 실행되는 시간을 최대한 줄였다.
- 락의 범위를 축소한 결과 락을 확보한 상태에서 실행돼야 하는 명령어의 수를 줄일 수 있엇다.
- 암덜의 법칙을 통해보면 순차적으로 처리돼야 하는 코드의 양이 줄어드는 효과가 있기때문에 어플리케이션의 확장성을 저해하는 요소를 줄이는 결과도 기대할 수 있다.
- AttributeStore의 변수가 하나이기때문에 스레드 안정성 위임 방법을 사용해서 개선가능
- HashMap 대신 스레드 안정성이 확보된 클래스(ConcurrentHashMap,Hashtable,synchronizedMap)를 사용하면 스레드 안정성을 모두 attributes 애 위임 할 수 있다.
    - 동기화 해줄 필요 없으므로 락을 점유하는 시간 최소화하는 셈이다.
- synchronized 블록 줄일수록 확장성 늘릴수 있지만 단일 연산으로 실행돼야할 명령어까지 synchronized 블록밖으로 빼내거나 해서는 안된다.
- synchronized 블록을 두개이상 쪼개는 일도 어느 한도를 넘어서면 성능측면에서 악영향을 미칠수 있다.
- 일반적으로 볼때 대기상태에 들어 갈 수 있는 연산뿐만아니라 아주 작은 연산까지 최대한 synchronized 블록 밖으로 빼내는 정도로 충분하다.

### 락 정밀도 높이기
- 락을 점유하고 있는 시간을 최대한으로 줄이고 따라서 락을 확보하기 위해 경쟁하는 시간을 줄일수 있도록 또다른 방법으론 바로 스레드에서 해당 락을 덜 사용하도록 변경하는 방법이 있다.
- 릭 분할과 락 스트라이핑 방법이 있는데 두가지 모두 하나의 락으로 여러개의 상태 변수를 한번에 묶어두지 않고, 서로 다른 락을 사용해 여러개의 상태 변수를 한번에 묶어 두지 않고 서로 다른 락을 사용해 여러개의 독립적인 상태 변수를 각자 묶어두는 방법이다.
- 두가지 기법을 호라용하면 락으로 묶이는 프로그램의 범위를 조밀하게 나누는 효과가 있으며, 따라서 결국 애플리케이션의 확장성이 높아지는 결과를 기대 할 수 있다. 하지만 반대로 락의 개수가 많아질수록 데드락이 발생할 위험도가 높아진다는 점을 주의해야한다.

~~~java
@ThreadSafe
public class ServerStatus {
    @GuardedBy("this")
    public final Set<String> users;
    @GuardedBy("this")
    public final Set<String> queries;
    
    public synchronized void addUser(String u) { users.add(u);}

    public synchronized void addQuery(String q) { queries.add(q);}

    public synchronized void removeUser(String u) { users.remove(u);}

    public synchronized void removeQuery(String q) { queries.remove(q);}
}

~~~
- 락 분할 대상

~~~java
@ThreadSafe
public class ServerStatus {
    @GuardedBy("this")
    public final Set<String> users;
    @GuardedBy("this")
    public final Set<String> queries;

    public void addUser(String u) {
        synchronized (users) {
            users.add(u);
        }

    }

    public void addQuery(String q) {
        synchronized (queries) {
            queries.add(q);
        }
    }

    public void removeUser(String u) {
        synchronized (users) {
            users.remove(u);
        }
    }

    public void removeQuery(String q) {
        synchronized (queries) {
            queries.remove(q);
        }
    }
}

~~~
- 락이 분할 된 ServerStatus 클래스
- 각 상태 변슈를 각자의 락으로 동기화하도록 해보자. 이렇게 락을 분할하고 나면, 락의 정밀도가 높아졌다고 볼수 있고 분할하기전에 정밀도가 적은 방법보다 대기상태에 들어가는 경우가 크게 줄어든다.
- 락을 하나에서 둘로 분할하는 방법은 경쟁 조건가 아주 심하지는 않지만 그래도 어느정도 경쟁이 발생하고 있는 경우에는 가장 큰 효과를 볼 수 있다.
- 어느정도 경쟁이 발생하는 상황에서 락을 두개 이상으로 분할하면 대부분의 동기화 블록에서 락 경쟁이 일어나지 않도록 할 수 있으며, 따라서 처리량과 확장성의 측면에서 큰 이득을 얻을 수 잇다.

### 락 스트라이핑
- 락 분할 방법은 때에 따라 독립적인 객체를 여러가지 크기의 단위로 묶어내고, 묶인 블록 단위로 락을 나눈 방법을 사용 할 수 도 있는데, 이런 방법을 락 스트라이핑이라고 한다.
- 이런 방법을 락 스트라이핑이라고 한다.
- 예를 들어 ConcurrentHashMap 클래스가 구현된 소스 코드를 보면 16개의 락을 배열로 마련해두고 16개의 락 각자가 전체 해시 범위의 1/16에 대한 락을 담당하낟.
- 따라서 N 번째 해시 값은 락 배열에서 N mod 16의 값에 해당하는 락으로 동기화된다. ConcurrentHashMap 에서 해시 함수가 적당한 수준이상으로 맵에 들어 있는 항목을 분산시켜 준다는 가정하에 락 경쟁이 발생할 확률을 1/16으로 낮추는 효과가 있다.
- ConcurrentHashMap은 최대 16에 스레드에서 경쟁없이 동시에 맵에 들어 있는 데이터를 사용 할 수 있도록 구현돼 있는 셈이다.
- 락스트링핑을 사용하다 보면 여러개의락을 사용하도록 쪼개ㅑ 놓은 컬렉션 전체를 한꺼번에 독점적으로 사용해야 할 필요가 있을 수 도 있는데, 이런 경우에는 단일 락을 사용할때보다 동기화시키기가 어렵고 자원도 많이 소모한다는 단점이있다.
- ConcurrentHashMap 클래스에서 해시 공간의 크기를 늘리고 해시 함수를 새롭게 적용하는 작업과 같이 간혹 전체 컬렉션을 독점적으로 사용해야하는 경우가 생긴다. 이런 경우에는 보통 쪼개진 락을 전부 확보한 이후에 처리한도록한다.

~~~java
public class StripedMap {
    // 동기화 정책: buckets[n] 은 locks[n%N_LOCKS] 락으로 동기화 한다.
    private static final int N_LOCKS = 16;
    private final Node[] buckets;
    private final Object[] locks;

    private static class Node {...
    }

    public StripedMap(int numbuckets) {
        this.buckets = new Node[numbuckets];
        this.locks = new Object[N_LOCKS];
        for (int i = 0; i < N_LOCKS; i++) {
            locks[i] = new Object();
        }
    }

    private final int hash(Object key) {
        return Math.abs(key.hashCode() % buckets.length);
    }

    public Object get(Object key) {
        int hash = hash(key);
        synchronized (locks[hash % N_LOCKS]) {
            for (Node m = buckets[hash]; m != null; m = m.next)
                if(m.key.equals(key))
                    return m.values;
        }
        return null;
    }
    
    public void clear(){
        for(int i=0; i<buckets.length; i++){
            synchronized (locks[i % N_LOCKS]){
                buckets[i] = null;
            }
        }
    }
}
~~~
- 락 스트리핑을 사용하는 해시 기반의 맵
- 락 스트라이핑을 사용하다 보면 여러개의 락을 사용하도록 쪼개놓은 컬렉션를 한꺼번에 독점적으로 사용해야 할 필요가 있을 수 있는데, 이런 경우에는 단일락을 사용할때보다 동기화시키기가 어렵고 자원도 많이 소모한다는 단점도 있다.
- 대부부의 작업을 처리할때는 쪼개진 하나만을 확보하는 것으로도 충분하지만 ConcurrentHashMap 클래스에서 해시 공간의 크기를 늘리고 해시 함수를 새롭게 적용하는 작업과 같이 간혹 전체 컬렉션을 독점적으로 사용해야 하는 경우가 생긴다. 이런경우 보통 쪼개진 락을 전부 확보한 이후 처리하도록 구현한다.
- N_LOCKS 만큼 락을 생성하고, N_LOCKS개의 락이 각자 범위에 해당하는 해시 공간에 대한 동기화를 담당한다. get메소드와 같은 대부분의 메소드는 N_LOCKS개의 락을 한꺼번에 모두 확보해야하는 경우가 있긴 하지만, clear 메소드에 구현돼 있는 것과 같이 N_LOCKS개의 락을 한꺼번에 확보하지 않고 처리할 수 잇는 방법이 있을 수 도 있다.

### 핫 필드 최소화
- 락 분할 방법과 락 스트라이핑은 여러 개의 스레드가 각자 방해 받지 않으면서 독립적인 데이터를 사용할 수 있도록 해주기때문에 어플리케이션의 확장성을 높여준다.
- 어플리케이션의 내부를 살펴봣을때 락으로 동기화 시킨 데이터에 대한 경쟁보다 락 자체에 대한 경쟁이 더 심한 상태인 경우에 락 분할 방법으로 확장성에 이득을 얻을수 있다.
- 예를 들어 자주 계신하고 사용하는 값을 캐시에 저장해두도록 최적화 한다면 확장성을 떨어트릴수박에 없는 핫필드가 나타난다.
- HashMap 클래스를 구현한다고하면 맵 내부에 들어있는 항목개수를 세는 size 메소드 어떻게 구현해야할지 선택
    - size 메소드를 호출할때마다 매번 새로 계산하는방법
    - 항목의 개수 카운터를 두고 항목이 추가되거나 제거될때마다 카운트를 증가시키거나 감소시키는 방법
        - 모든 연산을 수행할때마다 한번씩 사용해야 하는 카운터 변수와 같은 부분을 핫필드라 부른다.
- JDK 포함된 ConcurrentHashMap 클래스는 전체 카운터를 하나의 변수에 두지 않고, 락으로 분배된 각 부분마다 카운터 변수를 따로 두고 관리하면서 size 메소드를 호출하면서 각 카운터 변수의 합을 알려주는 방법을 사용한다.
- 즉 ConcurrentHashMap은 모든 항목의 개ㅔ수를 하나씩 세는 대신 각 락이 담당하는 부분마다 카운터를 두고 있으며, 해당 부분은 락으로 이미 동기화돼 있기 때문에 추가적인 락을 사용할 필요가 없다.

### 독점적인 락을 최소화하는 다른 방법
- 락 경쟁 때문에 발생하는 문제점을 줄일 수 있는 또다른 방법은 바로 좁더 높은 병렬성으로 공유된 변수를 관리하는 방법을 도입해 독점적인 락을 사용하는 부분을 줄이는 것이다.
- 예를 들어 병렬 컬렉션 클래스를 사용하거나 읽기 쓰기 락을 사용하거나 불면 객체를 사용하고 단일 연산 변수를 사용하는 등의 방법이 여기에 해당한다.
- 단일 연산 변수를 사용하면 통계값을 위한 카운터 변수나 일련변수 생성모듈, 링크로 구성된 데이터 구조ㅓ에서 첫번째 항목을 가리키는 링크와 같은 핫필드가 존재할때 핫필드의 값을 손쉽게 변경 할 수있개ㅔ 해준다.
- 딘일 클래스는 숫자나 객체애 대한 참조등을 대상으로 굉장히 정밀도가 높은 단일연산 기능을 제공하며, 그 내부적으로는 CPU 프로세서에서 제공하는 저수준의 병렬처리기능을 활용하고 있다.
- 작성중인 클래스 내부에 다른 변수와의 불변조건에 관하여 하지 안ㅅㅎ는 핫필드가 몇개 정도 있다면 해당하는 핫필드를 단일연산로 변경하는 것만으로도 확장성에 이득을 볼 수 있다.

### CPU 활용도 모니터링
- cpu 충분히 활용하지 못한다고 볼수 있다. 일반적으로 몇가지 원인을 생각해 볼수 있다.
- 부하가 부족하다.
- I/O 제약
- 외부제약 사항
- 락 경쟁

### 객체 풀링은 하지 말자
- 병렬 어플리케이션에서는 객체 풀링을 사용했을때 훨씬 사용 많은 비용을 지불 해야 할 수 도 있다.
- 스레드 내부에서 필요로 하는 객체를 새로 생성할때는 힙 데이터 구조를 사용할땨ㅐ 동기화 해야 하는 부분을 건너띨 수 있도록 스레드 내부의 할당 블록을 사용하기때문에 스레드 간에 조율해야할 일이 거의 없다고 볼 수 있다.
- 그런데 이와 같은 스레드에서 공통의 객체 풀 하나를 놓고 객체를 재사용한다면 풀에 들어 있는 객체를 사용하고자 할때 모종의 동기화 방법을 사용해야 하며 , 따라서 락을 확보하기 위해 스레드가 대기 상태에 들어가야할 가능이 생긴다.
- 그런데 스레드가 락 경쟁에 밀려 대기 상태에 들어가 기다리는 작업 흐름은 메모리에 객체를 할당하는 일보다 훨씬 자원을 많이 소모한은 일이기 때문에 아주 작은양이라 해도 객체 풀 때문에 발생하는 락 경쟁 상황은 어플리케이션 확장성에 지대한 영향을 미치는 병목이 될수 있다.
- 객체 풀 역시 성능을 최적화 할수 있는 방법 가운데 하나라고 생각하기도 하지만, 반대로 확장성에는 심각한 문제를 일으킬 수 있다. 객체풀은 그것만을 적절한 용도가 있으며, 성능을 최적화 하는데 사용하기에는 그다지 적절한 방법이 아니다.
-  **스레드는 동기화 하는 것보다 메모리에 객체를 할당하는 일이 훨씬 부담이 적다.**

## 11.5 예제: Map 객체의 성능 분석
- 단일 스레드 환경에서 ConcurrentHashMap 은 동기화된 HashMap 보다 약간 성능이 빠르다.
- 병렬 처리 환경에서는 ConcurrentHashMap의 성능이 빛을 발한다.
- ConcurrentHashMap의 가장 많이 사용하는 기능이 현재 맵 내부에 갖고 있는 값을 찾아내 가져가는 연산이라고 가정하고 있으며, 여러개의 스레드에서 get 메소드를 연달아 호출하는 경우에 가장 빠른 속도를 낸다.
- 동기화된 HashMap 클래스가 속도가 떨어지는 가장 큰 이유는 맵 전체가 락으로 동기화돼 있다는 점이고, 한번에 하나의 스레드만 맵을 사용 할 수 있다.
- ConcurrentHashMap은 대부분의 읽기 연산에는 락을 걸지 않고 있으며 쓰기 연산과 일부 읽기 연산에는 락 스트라이핑을 활용하고 있다. 이런 기법에 힘입어 대부분이 경우 대기 상태에 들어가지 않고도 다수의 스레드가 동시에 ConcurrentHashMap의 기능을 사용 할 수 있다.

  ![KakaoTalk_Photo_2021-06-12-12-50-07](https://user-images.githubusercontent.com/38197944/121764162-bd76e100-cb7c-11eb-94f8-71b193279bb6.jpeg)
- ConcurrentHashMap, ConcurrentSkipListMap은 애초에 설계할때 멀티 스레드 환경에서 사용하는 것을 목표로 만들어졌고, SynchronizedMap 을 활용해 동기화 시킨 HashMap, TreeMap 은 아주 단순하게 강제로 동기화를 맞춘것이라고 볼수 있다.
- 성능을 측정하는 각 경우마다 N개의 스레드가 임의의 키를 선택한다음 그 키에 해당하는 값을 맵에서 읽어오는 단순한 코드 반복한다.
- ConcurrentHashMap, ConcurrentSkipListMap에 대한 결과를 보면 스레드 수가 늘어남에 따라 성능이 잘 따라와 준다는 사실을 알 수 있음, 스레드 개수가 늘어남에 따라 처리량도 같이 늘고 있다.
- SynchronizedMap으로 동기화 된 맵이 보여주는 성능 수치는 그다지 좋지않다.
- 단일 스레드로 동작할때는 ConcurrentHashMap과 대등한 속도를 보여주지만 경쟁 조건이 발생하지 않는 상황에서 경쟁이 발생하는 상황으로 넘어가면 성능이 급격하게 저하하는것을 볼수 있다.
- 락 경쟁을 제대로 막지 못하는 경우에 흔히 발생한다.
- 경쟁이 많이 발생하지 않는 상황에서는 연산하는데 필요한 시간이 실제 작업에 필요한 시간과 크게 차이 나지 않으며, 스레드가 추가될수록 성능도 함께 증가한다. 하지만 한번 경쟁이 발생하기 시작하면 연산에 필요한 시간의 대부분이 컨텍스트 스위칭과 스케줄링에 필요한 대기시간으로 소모되며, 스레드를 추가한다해도 성능을 거의 끌어올리지 못한다.

## 11.6 컨텍스트 스위치 부하 줄이기
- 실행과 대기의 두가지 상태를 옮겨다니는것을 컨텍스트 스위치라고한다.
- 서버 어플리케이션에서 대기 상태에 들어가기 쉬운 경우
    - ex) 로그 메세지 생성하는 경우
- 컨텍스트 스위치 횟수를 줄이면 서버의 처리량에 어떤 변화가 있는지 2가지 로그 출력 방법 사용했을때 비교
- println 문장을 호출하는 경우

~~~java
public class LogWriter {
    private final BlockingQueue<String> queue;
    private final LoggerThread logger;

    public LogWriter(Writer writer) {
        this.queue = new LinkedBlockingQueue<>(CAPACITY);
        this.logger = new LoggerThread(writer);
    }
    
    public void start(){
        logger.start();
    }
    
    public void log(String msg) throws InterruptedException{
        queue.put(msg);
    }
    
    private class LoggerThread extends Thread{
        private final PrintWriter writer;
        ...
        public void run(){
            try{
                while (true)
                    writer.println(queue.take());
            }catch (InterruptedException ignored){}
            finally {
                writer.close();
            }
        }
    }
}
~~~
- 229쪽 LogWriter 와 같은 모습, 로그 출력 작업 백그라운드 스레드에 의해 진행되며, 실제로 로그 메세지를 출력하고자 했던 스레드는 실제로 로그를 출력하지 않는다.

- 2가지 방법 성능차이 남
    - 출려기되는 로그 메세지의 양이나 로그메세지를 몇개의 스레드에서 출력하는
    - 컨텍스트 스위치를 하는데 얼마만큼의 자원이 필요한지 등에 의해 차이가 발생한다.
- 로그 출력 기능에 걸리는 시간은 항상 I/O 스트림 클래스와 관련된 모든 작업 시간을 포함한다.
- 즉 I/O 연산이 대기 상태에 들어가면 해당 스레드가 대기중인 시간까지 전체 작업 시간에 포함된다.
- 그러면 운영체제는 I/O 작업이 끝날떄까지 해당 스레드를 대기상태에 집어 넣는다.
- 스레드에 따라 달라질수 있음, 그렇기 때문에 서비스 시간이 늘어남
- 다수의 스레드가 동시다발적으로 로그메세지를 출력하고자 한다면 메세지를 출력하는 출력스트림 객체에 대한 락을 두고 경쟁이 발생할 수 있다.
- 그러면 블로킹 I/O의 경우 같이 스레드가 락을 확보하기 위해 대기상태에 들어가면서 컨텍스트 스위치가 발생하는 결과가 나타난다.
- 로그 메세지를 즉시 출력하는 방법은 I/O 연산과 스트림에 대한 락에 직접적으로 연결돼 있으며, 따라서 컨텍스트 스위치가 빈번하게 발생할 가능성이 높고 서비스 시간은 점점 늘어난다.
- 서비스 시간이 길어진다는 애기는 바로 누군가가 서비스 결과를 얻기 위해 오랫동안 기다리는다는 말과 같다.
    - 서비스 시간이 길어질수록 락 경쟁을 심화시킨다는 점
- 락을 오랫동안 확보하고 있을수록 락에 대한 경쟁이 발생할 가능성이 높아지기 때문에 락을 확보하고 있는 시간을 최대한 줄여야한다.
- 락을 놓고 경쟁하고 있다는 말을 컨텍스트 스위치가 많이 일어나고 있다는 말과 같음. 전반적인 성능이 저하 될수 밖에없다.
- 병렬 처리 시스템은 대부분의 락을 대상으로 경쟁이 발생하지 않는 경우에 훨씬 높은 성능을 보여준다.

- 요청을 처리하는 스레드의 외부로 I/O 작업을 뽑아내는 방법은 요청을 처리하는 평균시간을 줄여주는 좋은방법이다.
    - log 메소드를 호출하는 스레드는 더이상 출력 스트림에 대한 락을 확보할 필요도 없고 I/O 완료 될때 까지 대기하지 않아도 된다.
    - 출력할때 로그 메세지를 큐에 쌓아두는 즉시 리턴돼 본연의 작업을 진행 할 수 있다.
    - 반대로 메세지큐를 사용하기 위한 경쟁이 발생할 가능성이 높긴하지만, 메세지르 큐를 쌓는 put연산이 실제로 출력스트림에 메세지를 출력하는 연산보다 훨씬 가벼운 연산
    - 따라서 실제 상황에서 로그를 출력하고자 할때 스레드가 대기 상태에 들어갈 일이 거의 없고, 로그메세지를 출력하느라 컨텍스트 스위치가 발생할 확률도 줄일수 있다.
    - 작업이 실제로 처리되는 위치를 옮겼고, 로그 관련 I/O 작업을 모두 단 하나의 스레드에서 처리하도록 넘기고 있기 때문에 로그 출력 스트림을 공유하지 않아도 되고, 따라서 대기 상태에 들어 갈 수 있는 원인을 방지하고 있다.
- 이런 구조를 갖춰두면 스케줄링, 컨텍스트 스위칭, 락관리와 같은 각 부분에서 사용하는 자원의 양을 크게 줄일수 있기때문에 전반적인 성능을 높일 수 있다.

- 로그를 출력하고자 요청하는 다수의 스레드에서 발생할 I/O 연산을 단하나의 스레드에서 처리하도록 한군데로 몰아두는 일
    - ex) 불을 끄고자 할때 서로 양동이를 들고 뛰어다니는 경우, 줄을 맞춰 서서 양동이를 넘겨주는 방법
- 스레드 입장에서는 대기상태에 들어가거나 컨텍스트 스위치가 일어나는일이 원래 작업을 처리하는데 상당한 방해가 된다.

요약
- 멀티스레드를 사용하는 큰 이유중 하나가 바로 다중 CPU 하드웨어를 충분히 활용하고자 하는것이다.
- 암달의 법칙에 따르면 어플리케이션의 확장성은 반드시 순차적으로 실행돼야만 하는 코드가 전체에서 얼마만큼의 비율을 차지하냐에 달렸다고함
- 자바프로그램의 내부에서 순차적으로 처리해야만 하는 가장 주요한 부분은 바로 독점적인 락을 사용하는 부분이기 때문에 락으로 동기화하는 범위를 세분화해 정밀도를 높이거나 락을 확보하는 시간을 최소한으로 줄이는 기법을 사용해 락으 최소한만 사용해야한다.
- 그리고 독점적인 락 대신 독점적이지 않은 방법을 사용하거나 대기 상태에 들어가지 않는 방법을 사용하는 것도 중요하다.

## 참고
책 자바 병렬 프로그래밍 11장